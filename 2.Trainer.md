# Transformers.Trainer

: https://bo-10000.tistory.com/154

1. Train Process

- trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=train_dataset if training_args.do_train else None,
  eval_dataset=eval_dataset if training_args.do_eval else None,
  tokenizer=tokenizer,
  compute_metrics=compute_metrics,
  data_collator=default_data_collator,
  )

Data collator will default to DataCollatorWithPadding, so we change it.

- model : CasualLm, MaskedLM
- args : training_args
  : https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/trainer#transformers.TrainingArguments

    <div>
    '''
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    '''

            class ModelArguments:

                model_name_or_path: Optional[str] = field(
                    default=None,
                    metadata={
                        "help": "The model checkpoint for weights initialization."
                        "Don't set if you want to train a model from scratch."
                    },
                )
                model_type: Optional[str] = field(
                    default=None,
                    metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
                )
                config_name: Optional[str] = field(
                    default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
                )
                tokenizer_name: Optional[str] = field(
                    default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
                )
                cache_dir: Optional[str] = field(
                    default=None,
                    metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
                )
                use_fast_tokenizer: bool = field(
                    default=True,
                    metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
                )
                model_revision: str = field(
                    default="main",
                    metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
                )
                use_auth_token: bool = field(
                    default=False,
                    metadata={
                        "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
                        "with private models)."
                    },

    </div>

- train_dataset : <- lm_datasets['train']
  :huggingface.dataset의 변형들이 들어감. torch.utils.data.Dataset이라고 생각해도 무관하다.
  : https://github.com/huggingface/datasets

  <div>

      datasets = load_dataset(
              extension,
              data_files=data_files,
              cache_dir=model_args.cache_dir
      )

      tokenized_datasets = datasets.map(
          tokenize_function,
          batched=True,
          num_proc=data_args.preprocessing_num_workers,
          remove_columns=column_names,
          load_from_cache_file=not data_args.overwrite_cache,
      )

      lm_datasets = tokenized_datasets.map(
          group_texts,
          batched=True,
          num_proc=data_args.preprocessing_num_workers,
          load_from_cache_file=not data_args.overwrite_cache,
      )

</div>

- https://junbuml.ee/transformers-trainer-deep-inside
